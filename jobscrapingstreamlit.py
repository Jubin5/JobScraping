# -*- coding: utf-8 -*-
"""JobScrapingStreamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U1hh6TlkZKqPUdCh0WsvISEVwvyhWz4I
"""


import streamlit as st
import pandas as pd
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import re
import requests
from bs4 import BeautifulSoup
import time

# ------------------------
# Web Scraping Function
# ------------------------
def scrape_karkidi_jobs(keyword="data science", pages=2):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.content, "html.parser")

        job_blocks = soup.find_all("div", class_="ads-details")
        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True)
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x).get_text(strip=True)
                location = job.find("p").get_text(strip=True)
                experience = job.find("p", class_="emp-exp").get_text(strip=True)
                key_skills_tag = job.find("span", string="Key Skills")
                Skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company,
                    "Location": location,
                    "Experience": experience,
                    "Summary": summary,
                    "Skills": Skills
                })
            except Exception:
                continue

        time.sleep(1)

    return pd.DataFrame(jobs_list)

# ------------------------
# Preprocessing Function
# ------------------------
def preprocess_skills(df):
    df["Skills"] = df["Skills"].apply(lambda x: x.lower())
    df["Skills"] = df["Skills"].apply(lambda x: re.sub(r"[^a-z, ]", "", x))
    df["Skills"] = df["Skills"].apply(lambda x: ", ".join(set(x.split(", "))))
    return df

# ------------------------
# Clustering Function
# ------------------------
def cluster_jobs(df, n_clusters=5):
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(df["Skills"])
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(X)
    df["Cluster"] = kmeans.labels_
    return df

# ------------------------
# Streamlit UI
# ------------------------
st.title("Karkidi Job Scraper & Clusterer")

keyword = st.text_input("Enter job search keyword:", value="data science")
pages = st.slider("Number of pages to scrape:", min_value=1, max_value=5, value=2)
clusters = st.slider("Number of clusters:", min_value=2, max_value=10, value=5)

if st.button("Scrape and Cluster Jobs"):
    with st.spinner("Scraping job data..."):
        df = scrape_karkidi_jobs(keyword=keyword, pages=pages)

    if not df.empty:
        df = preprocess_skills(df)
        df = cluster_jobs(df, n_clusters=clusters)

        st.success(f"Successfully scraped and clustered {len(df)} job postings.")
        st.dataframe(df)

        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("Download CSV", csv, "clustered_jobs.csv", "text/csv")
    else:
        st.warning("No jobs found.")
